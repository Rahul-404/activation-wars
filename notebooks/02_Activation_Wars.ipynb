{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8c2b8d36",
   "metadata": {},
   "source": [
    "![image.png](../results/activation_functions.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a2439bd",
   "metadata": {},
   "source": [
    "# Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3716aaa0",
   "metadata": {},
   "source": [
    "$Q.$ What is a activation function ?\n",
    "\n",
    "$\\longrightarrow$ An activation function decides weather a neuron should be activated or not."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6880f70f",
   "metadata": {},
   "source": [
    "There are 2 types of activations Linear and Non-Linear:\n",
    "\n",
    "- Linear 5% of the time will be used\n",
    "\n",
    "- Non-Linear 95% of the time will be used\n",
    "\n",
    "$\\longrightarrow$ Real-time data is non-linear\n",
    "\n",
    "$$\n",
    "Activating \\longrightarrow \\text{(Mathematical Transformation)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94b21b7",
   "metadata": {},
   "source": [
    "$Q.$ Why do neural networks needs an **Activation Functions** ?\n",
    "\n",
    "$\\longrightarrow$ Well, the purpose of an activation function is to add non-linearity to the neural network\n",
    "\n",
    "Activation functions introduces and additional step at each layer during the forward propogation but its computation is worth it.\n",
    "\n",
    "Here is Why...\n",
    "\n",
    "Let's suppose we have a neural networking without the activation function.\n",
    "\n",
    "In that case, every neuron will only performing a linear transformation on the inputs using the weights and biases, It's **because it doesn't matter how many hidden layers we attach in the neural network**, *\"all layers will behave in the same way because the composition of two linear function is a linear function itself\"*.\n",
    "\n",
    "Although, the neurak betwork becomes simpler, learning any complex task is impossible, and our model would be just a linear regression model."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bde7840b",
   "metadata": {},
   "source": [
    "# * Linear activation function has two major problems "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97ee19ae",
   "metadata": {},
   "source": [
    "- it is not possible to use back propogation as the derivative of the function is a constant and has no relation to the input $x$\n",
    "\n",
    "- all layers of the neural network will collapse into one if a linear activation function is used NO matter the number of layers in the neural network , the last layer will still be a linear function of the first layer, so essentially a linear activation function turns the neural network into just one layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e8ecda8",
   "metadata": {},
   "source": [
    "> image of neural nets"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd38da6c",
   "metadata": {},
   "source": [
    "1] Before the output in all layes\n",
    "\n",
    "2] output layers"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "adb1bdbe",
   "metadata": {},
   "source": [
    "### Binary Classification\n",
    "\n",
    "> Last Activation $\\longrightarrow$ **Sigmoid**\n",
    "\n",
    "> Last Function $\\longrightarrow$ **BCE**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce5d0034",
   "metadata": {},
   "source": [
    "**Non-linear Activation Functions**\n",
    "\n",
    "- The linear activation function shown above is simply a linear regression model, Because of its limited powers this does not allow the model to create a complex mappings between the networks input & outputs\n",
    "\n",
    "- Non-linear functions solve the following limitations of linear activation functions\n",
    "\n",
    "    - they allow backpropogation because now the derivative function would be related to this input , and its possible to go back and understand which weight in the input neurons can provide a better prediction.\n",
    "\n",
    "    - they allow the stacking of multiple layers of neurons as the output would now be a non-linear combination of input passed throught multiple layers. Any output can be represented as a functional computation in a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c8465f2",
   "metadata": {},
   "source": [
    "## 1] Sigmoid / Logistic Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5f86cd",
   "metadata": {},
   "source": [
    "![sigmoid](../results/sigmoid_plot.png)\n",
    "\n",
    "> Figure: The sigmoid function maps any real-valued input to a range between 0 and 1. It's commonly used as an activation function in neural networks."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "983a171c",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{Real Value} [- \\infty, \\infty] \\longrightarrow \\sigma (x) \\longrightarrow \\text{range}[0, 1] \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9fafa10",
   "metadata": {},
   "source": [
    "> larger input (more positive) $\\longrightarrow  1.0$\n",
    "\n",
    "> smaller input (more negative) $\\longrightarrow  0.0$ "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28574082",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{Sigmoid/Logistic} = f(x) = \\frac{1}{ 1 + e^{-x} }\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f14d0cfb",
   "metadata": {},
   "source": [
    "$Q.$ Why sigmoid/logistic activation function is used widely ?\n",
    "\n",
    "$ \\longrightarrow $ It is commonly used for models where we have to predict the probability as an output. Since probability of anything exits only between the range of 0 and 1., sigmoid is the right choice becase of its range\n",
    "\n",
    "- the function is differentiable and provides a smooth gradient i.e., preventing jumps in output values. this is represnted by as **S** shape of the sigmoid activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16e593d",
   "metadata": {},
   "source": [
    "**Limitation of sigmoid function are:**\n",
    "\n",
    "The derivative of sigmoid function is \n",
    "\n",
    "$$\n",
    "f^{'}(x) = \\text(Sigmoid(x)) \\cdot ( 1 - \\text(Sigmoid(x)))\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39f01c40",
   "metadata": {},
   "source": [
    "![sigmoid-derivative](../results/sigmoid_and_derivative.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5e773b2",
   "metadata": {},
   "source": [
    "- As we can see from above figure, the graduient values are only significant for range **-3** to **3** and the graph gets much flatter in other regions\n",
    "\n",
    "- It implies that for values greater than **3** or less than **-3**. the function will have very small gradients.\n",
    "\n",
    "- As the gradient value approaches zero. the network ceases to learn and sufferes from the vanishing gradient problem.\n",
    "\n",
    "- the output of the logistic function is not symmetric around zero, so the output of all the networks will be of the smae sign. this makes the training of the neural network more difficult and unstable."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9144bf25",
   "metadata": {},
   "source": [
    "![sigmoid-symmetry](../results/sigmoid_and_symmetry.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f93bf1f8",
   "metadata": {},
   "source": [
    "$$\n",
    "w_{new} = w_{old} - \\eta \\cdot \\frac{\\partial L}{\\partial w_{old}}\n",
    "$$\n",
    "$$\n",
    "w_{new} = w_{old} - 0.00000065\n",
    "$$\n",
    "$$\n",
    "w_{new} \\approx w_{old} \\quad \\text{No weight update}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0cf51f0b",
   "metadata": {},
   "source": [
    "In other words no training is going on. **This problem is called *<u>Vanishing Gradient Problem</u>*.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ace19a7",
   "metadata": {},
   "source": [
    "Loss will become **[stagnant](https://www.google.com/search?q=stagnant)** far from minima."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27da8361",
   "metadata": {},
   "source": [
    "### For a particular hidden layers:\n",
    "\n",
    "1] All gradients are $+VE$\n",
    "\n",
    "2] All gradients are $-VE$\n",
    "\n",
    "this are 2 conclusions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97a248e9",
   "metadata": {},
   "source": [
    "## 2] Tanh Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14e8f338",
   "metadata": {},
   "source": [
    "Similar to sigmoid , even has same **S-shape** with different output range [-1, 1]\n",
    "\n",
    "> input > 0 $\\longrightarrow$ output = 1.0\n",
    "\n",
    "> input < 0 $\\longrightarrow$ output = -1.0\n",
    "\n",
    "![tanh](../results/tanh_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b638b202",
   "metadata": {},
   "source": [
    "mathematically expressed as \n",
    "$$\n",
    "\\text{(Tanh)} = f(x) = \\frac{( e^{x} - e^{-x})}{(e^{x} + e^{-x})}\n",
    "$$\n",
    "\n",
    "- tanh majorly used in 2010, 2011, 2012 & 2013 widely, cause ReLU was not invented in this time."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "909c0ab1",
   "metadata": {},
   "source": [
    "**Advantages of tanh:**\n",
    "\n",
    "- The output of the *tanh* activation finction is zero centered, hence we can easily map the output values as strongly negative, neutral or strongly positive.\n",
    "\n",
    "- Usually used in hidden layers of a neural netwokr as its values lie between **-1** to **1**, there for the mean for the hidden layer comes out to be **0** or very close to zero, it helps in centering the data and makes learning much easier, have a look at the gradient of the *tanh* activation function to understand its limitations"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7af867f3",
   "metadata": {},
   "source": [
    "The derivative of tanh function is \n",
    "\n",
    "$$\n",
    "f^{'}(x) = \\frac{d}{dx}tanh(x) =  1 - tanh^{2}(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7fd299fb",
   "metadata": {},
   "source": [
    "![tanh-derivative](../results/tanh_derivative_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85169b27",
   "metadata": {},
   "source": [
    "- Still has vanishing gradient problem is there, but better than sigmoid , because zero centric, w.r.t sigmoid tanh is **more steeepest**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac2f437",
   "metadata": {},
   "source": [
    "## 3] SoftPlus Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9a8ec18",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "288789a7",
   "metadata": {},
   "source": [
    "## 4] ReLU Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ffa3a65",
   "metadata": {},
   "source": [
    "After 2013 in 2014 , ReLU is been introduced\n",
    "\n",
    "ReLU stands for **Rectified Linear Unit**\n",
    "\n",
    "Although it gives an impression of a linear function, ReLU has derivative function & allows for back propogation while simultaneously making it computationally efficient\n",
    "\n",
    "The main catch here is that ReLU function does not activate all the neurons at the same time.\n",
    "\n",
    "Neurons will only be deactivated if the linear transformation is less than **0**."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30fa1859",
   "metadata": {},
   "source": [
    "mathematically it can be represented as \n",
    "\n",
    "$$\n",
    "\\text{ReLU}(x)  = max(0, x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d816d7bf",
   "metadata": {},
   "source": [
    "![relu](../results/relu_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "359f476b",
   "metadata": {},
   "source": [
    "- **Negative** information is dummped\n",
    "- **Positive** information is passed\n",
    "- there is information loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83f16f09",
   "metadata": {},
   "source": [
    "**Advantages of ReLU:**\n",
    "\n",
    "- Since only a certain number of neurons are activated the ReLU function is far **more computationally efficient** when compared to the sigmoid and tanh functions.\n",
    "\n",
    "- ReLU accelerates the convergence of gradient descent towards the global minima of the **loss function** due to its linear, non-sturating property\n",
    "\n",
    "> `` ReLU works every where except output layer``\n",
    "\n",
    "**the limitations fixed by this function are:**\n",
    "\n",
    "- The `Dying ReLU` problem, \n",
    "$$\n",
    "f'(x) = \n",
    "\\begin{cases}\n",
    "1 & \\text{if } x > 0 \\\\\n",
    "0 & \\text{if } x \\leq 0\n",
    "\\end{cases}\n",
    "\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6efdefa0",
   "metadata": {},
   "source": [
    "![relu](../results/relu_derivative_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5ee0a0a",
   "metadata": {},
   "source": [
    "- The negative side of the graph makes the gradient value zero. Due to this reason, during the back propogation process, the weights and biases for some neurons are not updated. *( they get the update, but upadte is $ \\frac{\\partial L}{\\partial w} \\approx 0 $ , means same weight get updated to new weight)* thsi can create dead neurons which never get activated\n",
    "\n",
    "- above mentioned this is not a major **CON** , when we learn further concept of **DROP OUT**, then will manually do the same thing, it means that will set manually deactivate 20% of the neurons from network\n",
    "\n",
    "- 10% - 15% of neurons are deactivate when we do not use ReLU, we manually do **Regularization** using *DROP OUT* , where we tell network to please deactivate the neuron\n",
    "\n",
    "- ReLU solves the vanishing gardient problem for sure.\n",
    "\n",
    "\\begin{align*}\n",
    "z &= w \\cdot x + b \\\\\n",
    "  &= z * \\text{Activation Function} \\\\\n",
    "  &= z * 0 \\\\\n",
    "  &= 0 \\quad \\text{Dead Neuron}\n",
    "\\end{align*}\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dc3394e",
   "metadata": {},
   "source": [
    "-  All the negative input values become zero immediately which decreased the models ability to fit or train from the data properly\n",
    "\n",
    "Now lets try to understand from the dropout prespective."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70a19128",
   "metadata": {},
   "source": [
    "$Q.$ if i tell some neuron to be dead is that good or not ?\n",
    "\n",
    "$\\longrightarrow$ Dead neurons are also good.\n",
    "\n",
    "- if we are training face recognizer, and if we hide our one of the eye with hand, model should not say it is not human it should recognize , so yes we want it to learn all general patterns, so dead neurons are good."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "071b42d0",
   "metadata": {},
   "source": [
    "## 5] Leaky ReLU Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d27d45d4",
   "metadata": {},
   "source": [
    "Leaky ReLU is an improved version of ReLU function to solve the `Dying ReLU` problem as it has a small positive slope in the negative area."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09abd23a",
   "metadata": {},
   "source": [
    "$$\n",
    "\\text{Leaky ReLU} = max(0.01 * x, x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "48c51a11",
   "metadata": {},
   "source": [
    "$$\n",
    "f(x) =\n",
    "\\begin{cases}\n",
    "x & \\text{if } x > 0 \\quad \\text{(positive slope)} \\\\\n",
    "0.01 \\cdot x & \\text{if } x \\leq 0 \\quad \\text{(small positive slope)}\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Typically, $\\alpha = 0.01$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bea30f8c",
   "metadata": {},
   "source": [
    "![relu](../results/leaky_relu_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a38471",
   "metadata": {},
   "source": [
    "We can say the amount of information that is been taken into account it's low\n",
    "\n",
    "if we see function $max(0.1 * x, x)$ previously it was zero now it is taking probabily 10% of negative information in account.\n",
    "\n",
    "Earlier,\n",
    "\n",
    ">$\\text{if negative:}$\\\n",
    "> $\\quad$   dump $\\longrightarrow \\text{Dying ReLU problem}$\n",
    "\n",
    "Now,\n",
    "\n",
    ">$\\text{if negative:}$\\\n",
    "> $\\quad$   take 10% of -ve info $\\longrightarrow \\text{Solves Dying ReLU problem}$\n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a451cc95",
   "metadata": {},
   "source": [
    "**Advantage:**\n",
    "\n",
    "- The advantages of *Leaky ReLU* are same as that of *ReLU*, in addition to the fact that it does enable back propogation , even for negative input values.\n",
    "\n",
    "- By making this minor modification for negative input values, the gradient of the left side of the graph comes out to be a non-zero value. There for we would no longer encounter dead neurons in that regoin.\n",
    "\n",
    "Here is the derivative of the leaky ReLU function.\n",
    "\n",
    "**Leaky ReLU Derivative:**\n",
    "\n",
    "$$\n",
    "f^{`}(x) =\n",
    "\\begin{cases}\n",
    "1, & \\text{if } x > 0\\\\\n",
    "0.1 & \\text{if } x \\leq 0\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4c2036",
   "metadata": {},
   "source": [
    "![relu](../results/leaky_relu_derivative_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37a7c00f",
   "metadata": {},
   "source": [
    "**Limitations we face:**\n",
    "\n",
    "- The predictions may not be consistent for negative input values *[difference like $y= -0.9$ and $\\hat y = -0.7$ but if we fix the learning rate it will get fixed]*\n",
    "\n",
    "- The gradient for negative values is a small value that makes the learning of model parameters time-consuming.\n",
    "\n",
    "- here on some neurons we are taking $-ve$ quatient in account because of that those neurons will get update but smaller once , so it will make it slow, & it is not for over all neurons, because we are taking only 10% of the negative information in account thats why we are not facing `Dying ReLU` problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a55ed221",
   "metadata": {},
   "source": [
    "## 6] Parametric ReLU Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f446c22f",
   "metadata": {},
   "source": [
    "![relu](../results/prelu_activation_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8261a7ea",
   "metadata": {},
   "source": [
    "mathematically,\n",
    "\n",
    "$$\n",
    "\\text{Parametric ReLU} = f(x) = max(a * x, x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46e9180c",
   "metadata": {},
   "source": [
    "here main idea, is that negative gradient can have a actual impact on network ?\n",
    "\n",
    "$\\longrightarrow$ Parametric ReLU is another variant of ReLU that aims to save the problem of gradients becoming zero for the left half of the axis.\n",
    "This function provides the slope of the negative part of the function as an argument **a**, By performing backpropogation , the most appropriate value of **a** is learnt\n",
    "\n",
    "> \"Earlier in Leaky ReLU we were leking **10%** of the information $(-ve)$ to the network\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a62250a",
   "metadata": {},
   "source": [
    "$Q .$ How do we know only 10% of -ve informtaion is sufficient ?\n",
    "\n",
    "$\\longrightarrow$ So in Parametric ReLU we are passing argument **a** now we tell netrowk to figure it out what is good for it.\n",
    "\n",
    "Ex, NAS (Neural Architecture Search)\n",
    "\n",
    "**Time Taking:** it figures no of hidden layers needed\n",
    "\n",
    "$\\text{3 months} \\longrightarrow \\text{1 Year (Tearabytes of data)}$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "442e2ede",
   "metadata": {},
   "source": [
    "**Limitations:**\n",
    "\n",
    "- This functions limitation is that it may perform differently for different problems depending upon the values of the slope parameter **'a'** *[every time **a** will change]*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d084f0f",
   "metadata": {},
   "source": [
    "![relu](../results/prelu_derivative_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3d46a03",
   "metadata": {},
   "source": [
    "### 🔁 Leaky ReLU vs Parametric ReLU (PReLU)\n",
    "\n",
    "| Feature                      | **Leaky ReLU**                                                     | **PReLU (Parametric ReLU)**                    |\n",
    "| ---------------------------- | ------------------------------------------------------------------ | ---------------------------------------------- |\n",
    "| **Formula**                  | $f(x) = \\begin{cases} x & x > 0 \\\\ \\alpha x & x \\le 0 \\end{cases}$ | Same formula                                   |\n",
    "| **Negative Slope (α)**       | Fixed constant (e.g. 0.01 or 0.1)                                  | **Learnable parameter** (trained via backprop) |\n",
    "| **Learned during training?** | ❌ No (manually set)                                                | ✅ Yes (learns best α for each neuron)          |\n",
    "| **Advantage**                | Avoids dead neurons (ReLU problem)                                 | Potentially better performance by adapting     |\n",
    "| **Implementation**           | Simple to implement                                                | Slightly more complex (needs parameter update) |\n",
    "| **Use Case**                 | When you want a light ReLU fix                                     | When you want the model to adapt α             |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b66601bf",
   "metadata": {},
   "source": [
    "### 🔍 Example:\n",
    "\n",
    "- **Leaky ReLU:**\n",
    "\n",
    "You choose α = 0.01 manually and keep it fixed for all layers and neurons.\n",
    "\n",
    "- **PReLU:**\n",
    "\n",
    "The model starts with an α (e.g., 0.25), and during training it learns the best α for each unit/layer."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4440d0f2",
   "metadata": {},
   "source": [
    "### 📊 In Practice\n",
    "\n",
    "- **Leaky ReLU** is often enough for small or moderate models.\n",
    "- **PReLU** can offer a small accuracy gain — but may risk **overfitting** if you have too many αs and not enough data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df742e9e",
   "metadata": {},
   "source": [
    "## 7] Exponential Linear Unit (ELUs) Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68910d9",
   "metadata": {},
   "source": [
    "Exponential Linear Unit, or ELU for short is also a variant of ReLU that modifies the shape of the negative part of the function.\n",
    "\n",
    "ELU uses a log curve to define the negative values unlike the leaky ReLU & Parametric ReLU functions with a straight line"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4648c024",
   "metadata": {},
   "source": [
    "![relu](../results/elu_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf3cefd7",
   "metadata": {},
   "source": [
    "mathematically it can be represented as:\n",
    "\n",
    "$$\n",
    "ELU(x) =\n",
    "\\begin{cases}\n",
    "x & \\text{if } x \\ge 0\\\\\n",
    "\\alpha \\cdot (\\text{exp}(x) - 1) & \\text{if } x < 0\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e62c382",
   "metadata": {},
   "source": [
    "**Advantages:**\n",
    "\n",
    "ELU is a strong alternative for ReLU because of the following advantages:\n",
    "\n",
    "- ELU becomes smooth slowly until its output equal to **$- \\alpha$**, where as ReLU sharply smoothness\n",
    "\n",
    "- Avoids **Dead ReLU** problem by introducing log curve for negative values of input. it helps the network [nudge weights](https://www.google.com/search?q=nudge+weights+in+deep+learning) and biases in the right direction."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2849d2d",
   "metadata": {},
   "source": [
    "**Limitations:**\n",
    "\n",
    "- It increses the computational time because of the exponential operation included.\n",
    "\n",
    "- No learning of the **a** value takes place\n",
    "\n",
    "- Exploding gradient problem."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde5bbbb",
   "metadata": {},
   "source": [
    "Here is the derivative of the ELU function.\n",
    "\n",
    "**ELU Derivative:**\n",
    "\n",
    "$$\n",
    "f^{'}(x) =\n",
    "\\begin{cases}\n",
    "1 & \\text{if } x \\ge 0\\\\\n",
    "f(x) + \\alpha & \\text{if } x < 0\n",
    "\\end{cases}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfdd1cea",
   "metadata": {},
   "source": [
    "![relu](../results/elu_derivative_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c9e7fdb",
   "metadata": {},
   "source": [
    "> mostly all Deep Learning problems will be solved using **ReLU**, **Leaky ReLU** & **P-ReLU**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fd4d71c",
   "metadata": {},
   "source": [
    "## 8] Softmax Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4b11ca3",
   "metadata": {},
   "source": [
    "Before exploring in's and out;s of the softmax activation function, we should focus on it's building blocl _ the sigmoid/logistic activation function that works on calculating probability values."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ac63941",
   "metadata": {},
   "source": [
    "![relu](../results/softmax_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95fe7d46",
   "metadata": {},
   "source": [
    "The output of sigmoid function was on the range of **0 to 1**, which can be thought of as probability\n",
    "\n",
    "But-\n",
    "> This function faces some problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8a4a9a79",
   "metadata": {},
   "source": [
    "Let's suppose we have five output values of 0.5, 0.9, 0.7, 0.8 and 0.6 respectivel,. How can we make forward with it ? (means we can't pass judgement)\n",
    "\n",
    "**The answer is :** We can't "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b2a47c5",
   "metadata": {},
   "source": [
    "The above values don't make sence as the sum of all the classes/ output probabilities should be equal to 1.\n",
    "\n",
    "You see, the softmax function is described as a combination of multiple sigmoids, it calculates the relative probabilities, similar to the sigmoid/logistic activation function, the softmax function retunrns the probability of each class.\n",
    "\n",
    "It is most commonly used as an activation function of the for the last layer of the neural network in the case of multi-class classification mathematically it can be represented as:\n",
    "\n",
    "$$\n",
    "\\text{softmax}(z_i) = \\frac{\\text{exp}(z_i)}{\\sum_{j}^{} \\text{exp}(z_i)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e4371d",
   "metadata": {},
   "source": [
    "Lets go to simple example together,\n",
    "\n",
    "Assume that you have three classes, meaning that there would be three neurons in the output layers,\n",
    "Now suppose that your output from the neuron **[ 1.8, 0.9, 0.68]**\n",
    "\n",
    "Applying the softmax function over these values to give a probabilistic view will result in the following outcome: **[ 0.58, 0.23, 0.19]**\n",
    "\n",
    "The function returns **1** for the largest probability index while it returns **0** for the other two array indexes.\n",
    "\n",
    "Here, giving full weights index **0** and no weight to index **1** and index **2**, so the output would be the c;ass corresponding to the 1st neuron *(index 0)* out of three\n",
    "\n",
    "You can see now how softmax activation function make things easy for multi-class classification problems"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290c6d36",
   "metadata": {},
   "source": [
    "## 9] GELU Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ef55f22",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "74457ea3",
   "metadata": {},
   "source": [
    "## 10] Swish Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "320d1a59",
   "metadata": {},
   "source": [
    "> **Very good alternative of ReLU**\n",
    "\n",
    "Used in research if we are going for more number of hidden layers like $50 - 100 - 1000$, swich is the function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4928a743",
   "metadata": {},
   "source": [
    "it is a self-gated activation function developed by register function developed by researchers at *google*.\n",
    "\n",
    "Swish consistantly matches or outperforms ReLU activation function on deep networks applied to various challenging domains such as [image classification]() machine translation etc.\n",
    "\n",
    "This function is bounded below but unbounded above i.e, **Y** approaches to a constant values as **X** approached negative infinity byt **Y** approached to infinity as **X**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e4a7ead0",
   "metadata": {},
   "source": [
    "**Approaching Infinity**\n",
    "\n",
    "$$\n",
    "\\text{Swish} = f(x) = x \\cdot \\text{sigmoid}(x)\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1e3f24",
   "metadata": {},
   "source": [
    "![relu](../results/swish_plot.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e5a59d51",
   "metadata": {},
   "source": [
    "**Advantages of the swish activation finction over ReLU:**\n",
    "\n",
    "1. Swish is a smooth function that means that it does not abruptly change direction like ReLU does near $x=0$ , Rather it smoothly bends from **0** towards values **< 0** and then upwards again.\n",
    "\n",
    "2. Small negation values were zeroed out in ReLU activation function, How ever. those negative values may still be relevent for capturing patterns underlaying the data. Large negative values are zeroed out for reasons of sparsity making it a win-win situation.\n",
    "\n",
    "3. The swish function being an non-monotonous enhances the expression of input data and weight to be learnt."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7d4f282",
   "metadata": {},
   "source": [
    "## 11] Mish Activation Function"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b608984",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2112511a",
   "metadata": {},
   "source": [
    "### Conclusion:\n",
    "\n",
    "| Activation        | Keras Support                                               | Notes                          |\n",
    "| ----------------- | ----------------------------------------------------------- | ------------------------------ |\n",
    "| `sigmoid`         | ✔️ `activation='sigmoid'`                                   | Saturates at extremes          |\n",
    "| `tanh`            | ✔️ `activation='tanh'`                                      | Symmetric, but still saturates |\n",
    "| `relu`            | ✔️ `activation='relu'`                                      | Fast but can die               |\n",
    "| `leaky_relu`      | ✅ via `tf.keras.layers.LeakyReLU()`                         | Fixes dying ReLU               |\n",
    "| `parametric_relu` | ✅ via `tf.keras.layers.PReLU()`                             | Learns leak                    |\n",
    "| `elu`             | ✔️ `activation='elu'`                                       | Exponential tail for negatives |\n",
    "| `softmax`         | 🚫 **Not for hidden layers** — only use in **final output** |                                |\n",
    "| `swish`           | ✅ via `tf.keras.activations.swish`                          | Self-gated, newer activation   |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d59ffe20",
   "metadata": {},
   "source": [
    "## $Q.$ Why are deep neural network had to train ?\n",
    "\n",
    "$\\longrightarrow$  There are two challanges you might encounter when training your deep neuroal networks Let's discuss then in more detail\n",
    "\n",
    "**<u>Vanishing Gradient</u>**\n",
    "\n",
    "Like the sigmoid function, certain activation functions squish an sample input space into a small output space between **0 to 1**.\n",
    "\n",
    "There for, a large change in the input of the sigmoid function will cause a small change in the output , Hence the derivative becomes small for shallow networks , this isn't a big problem.\n",
    "\n",
    "**<u>Explodinghing Gradient</u>**\n",
    "\n",
    "Exploding gradients are problems where significant error gradient accumulate and result in very large updates to neural network model weights during training.\n",
    "\n",
    "An unstable network can result where there are exploding gradient and the learning cannot be completed.\n",
    "\n",
    "The values of the weughts can also become so large as to overflow and result in something could **NaN** values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9cb096f8",
   "metadata": {},
   "source": [
    "## $Q.$ How to choose the right activation function ?\n",
    "\n",
    "$\\longrightarrow$  You need to make your activation function for your output layers based on the type of prediction problem that your are solving specifically, the tye of predicted variables\n",
    "\n",
    "Here's what you should keep in mind\n",
    "\n",
    "<u>As a rule of thumb,</u> your can begin with using the ReLU activation and then move over to other activation functions if ReLU doesn't provide optimum results. \n",
    "And here are few otehr guidelines to help you out\n",
    "\n",
    "1. ReLU activation function should only be used in the hidden layers.\n",
    "\n",
    "2. Sigmoid/Logistic and Tanh functions should not be used in hidden layers as they make the model more susceptible for problem during training (due to vanishing gradients)\n",
    "\n",
    "3. Swish function is used in neural networks having a depth greater than **40 layers.**\n",
    "\n",
    "Finally, a few rules for choosing the activation function for your output layer based on the type of prediction problem that you are solving.\n",
    "\n",
    "**1. Regression :** Linear Activation Function\n",
    "\n",
    "**2. Binary Classification :** Sigmoid/ Logistic Activation Function.\n",
    "\n",
    "**3. Multi-Class Classification :** Softmax\n",
    "\n",
    "**4. Multi-Label Classification :** Sigmoid"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1e818f0",
   "metadata": {},
   "source": [
    "The activation function used in hidden layer is typically chosen based on the type of neural network architecture.\n",
    "\n",
    "**5. Convolutional Neural Network (CNN) :** ReLU activation function\n",
    "\n",
    "**6. Recurrent Neural Network (RNN) :** tahh and/or sigmoid activation function."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96a575fc",
   "metadata": {},
   "source": [
    "**Neural Networks Activation Function in a Nutshell :**\n",
    "\n",
    "*Key Takeways -*\n",
    "\n",
    "(1) Activation functions are used to introduce non-linearity in the network\n",
    "\n",
    "(2) A neural network will almost, always have the same activation function in all hidden layers, This activation function should be differentiable so that the parameters of the network are learned in back propogation\n",
    "\n",
    "(3) ReLU is the most commonly used activation function for hidden layers.\n",
    "\n",
    "(4) While selecting an activation function, you must consider the problems it might face : **Vanishing and Exploding Gradients**\n",
    "\n",
    "(5) Regarding the output layer we must always consider the expected value range if the predictions, if it can be any numeric value (as in case of the regression problem), you can use the linear activation function or ReLU.\n",
    "\n",
    "(6) Use softmax or signoid, function for the classification problems."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddd0236e",
   "metadata": {},
   "source": [
    "**[All researchers who were using SELU, GELU so on & so forth, they all moved to ReLU due to time complexity]()**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f157297d",
   "metadata": {},
   "source": [
    "### 🔢 Activation Functions Summary Table\n",
    "\n",
    "| **Activation Function** | **Year Introduced**                | **Equation**                                                                               | **Key Properties**                                                        |\n",
    "| ----------------------- | ---------------------------------- | ------------------------------------------------------------------------------------------ | ------------------------------------------------------------------------- |\n",
    "| **Sigmoid**             | 1960s                              | $\\sigma(x) = \\frac{1}{1 + e^{-x}}$                                                         | Smooth, maps to (0,1), suffers from **vanishing gradient**.               |\n",
    "| **Tanh**                | 1970s                              | $\\tanh(x) = \\frac{e^x - e^{-x}}{e^x + e^{-x}}$                                             | Zero-centered, maps to (−1, 1), also suffers from **vanishing gradient**. |\n",
    "| **ReLU**                | 2010                               | $f(x) = \\max(0, x)$                                                                        | Fast, sparse activations, but can suffer from **dying ReLU**.             |\n",
    "| **Softplus**            | 2000                               | $f(x) = \\ln(1 + e^x)$                                                                      | Smooth approximation of ReLU, always positive gradient.                   |\n",
    "| **Leaky ReLU**          | 2015                               | $f(x) = \\begin{cases} x & x > 0 \\\\ \\alpha x & x \\leq 0 \\end{cases}$                        | Fixes dying ReLU with small slope (e.g. $\\alpha = 0.01$) for negatives.   |\n",
    "| **PReLU**               | 2015                               | $f(x) = \\begin{cases} x & x > 0 \\\\ a x & x \\leq 0 \\end{cases}$                             | Like Leaky ReLU but **'a'** is **learned** during training.               |\n",
    "| **ELU**                 | 2016                               | $f(x) = \\begin{cases} x & x > 0 \\\\ \\alpha(e^x - 1) & x \\leq 0 \\end{cases}$                 | Smooth and helps mean activations closer to 0.                            |\n",
    "| **GELU**                | 2016 (popularized in 2018 by BERT) | $f(x) = x \\cdot \\Phi(x)$ <br> (where $\\Phi(x)$ is the CDF of standard normal distribution) | Combines properties of ReLU and dropout; **stochastic** and **smooth**.   |\n",
    "| **Swish**               | 2017 (by Google)                   | $f(x) = x \\cdot \\sigma(x)$                                                                 | Non-monotonic, smooth, performs better than ReLU on deeper models.        |\n",
    "| **Mish**                | 2019                               | $f(x) = x \\cdot \\tanh(\\ln(1 + e^x))$                                                       | Non-monotonic, smooth, high performance on NLP and vision tasks.          |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b23331e",
   "metadata": {},
   "source": [
    "### 📝 Notes:\n",
    "\n",
    "- **GELU** is often used in transformer architectures (e.g., BERT, GPT).\n",
    "- **Swish** and **Mish** are more recent and outperform ReLU in many scenarios but are computationally heavier.\n",
    "- **Softplus** is rarely used but is fully differentiable, unlike ReLU.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05e5c3b6",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
